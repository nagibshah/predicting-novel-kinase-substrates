---
title: "Group16_Project2"
group: 16
member: Nagib Shah / Ling Qi / Xinan Ma / Young Choi
author: "Ling Qi"
date: "19 September 2017"
output: html_document
---

# install packages 
```{r, echo=FALSE, warning=FALSE}

#install.packages("tibble") # this is for RSSL
#install.packages("RSSL")
#install.packages("upclass")
#install.packages("mclust")
#install.packages("caret")
#install.packages("lme4")
#install.packages("randomForest")
#install.packages("RecordLinkage")
#install.packages("knitr")
#install.packages("XLConnect")
#install.packages("class")
#install.packages("purrr")
#install.packages("mlbench")
#install.packages("adabag")
#install.packages("kknn")
#install.packages("fastAdaboost")
#install.packages("gbm")
#install.packages("lattice")
#install.packages("ggplot2")
#install.packages("Rcpp")
#install.packages("caret")
#install.packages("ddalpha")
#install.packages("tidyr")
#install.packages("pROC")

## try http:// if https:// URLs are not supported
#source("https://bioconductor.org/biocLite.R")
#biocLite("Biostrings")

#install if necessary
#source("http://bioconductor.org/biocLite.R")
#biocLite("seqLogo")

```

# call libraries 
```{r, warning=FALSE, echo=FALSE}
library(mclust)
library(caret)
library(plyr)
library(dplyr)
library(class)
library(e1071)
library(gbm)
library(ggplot2)
library(scales)
library(stringr)
library(tidyr)
library(mlbench)
library(Biostrings)
library(lattice)
library(ggplot2)
library(kknn)
```

# Read data

```{r}

#setwd("C:/Users/Ling/Documents/My Study/STAT5003/Assignment2/predicting-novel-kinase-substrates")

# load full data set
dt.Insulin <- read.delim("datasets/InsulinPhospho.txt")
#dim(dt.Insulin)

# load partially labeled data set
dt.Akt <- read.delim("datasets/Akt_substrates.txt", header = F)
dt.mTOR <- read.delim("datasets/mTOR_substrates.txt", header = F)   
#dim(dt.Akt)
#dim(dt.mTOR)

# map labled to the full data set 
dt.Insulin$Kinases[dt.Insulin$Identifier %in% dt.Akt$V1] <- "Akt"
dt.Insulin$Kinases[dt.Insulin$Identifier %in% dt.mTOR$V1] <- "mTOR"
unique(dt.Insulin$Kinases)
# generate a subset of the labelled data 
dt.labeled <- dt.Insulin[which(dt.Insulin$Kinases == 'Akt' | dt.Insulin$Kinases == 'mTOR'), ]

```

# load 2016 data 

```{r}

#setwd("C:/Users/Ling/Documents/My Study/STAT5003/Assignment2/predicting-novel-kinase-substrates")

# load prediction_2016 result

df.Akt <- read.csv(file="datasets/Akt_predictions2016.csv", header=TRUE, sep=",")
df.mTOR <- read.csv(file="datasets/mTOR_predictions2016.csv", header=TRUE, sep=",")
colnames(df.Akt)[1] <- "GeneSymbol"
colnames(df.mTOR)[1] <- "GeneSymbol"

df.Akt %>%
  mutate(Identifier = paste0(str_trim(str_to_upper(df.Akt$GeneSymbol), side="both"), ";",str_trim(df.Akt$Phosphorylation.site, side="both"), ";" )) %>%
  mutate(Full.model.predict = as.numeric(Full.model.predict))  -> df.Akt
names(df.Akt)[colnames(df.Akt)=="Full.model.predict"] <- "predictResult" 
head(df.Akt)

df.mTOR %>%
  mutate(Identifier = paste0(str_trim(str_to_upper(df.mTOR$GeneSymbol), side="both"), ";",str_trim(df.mTOR$Phosphorylation.site, side="both"), ";" )) %>%
  mutate(Full.model.predict = as.numeric(Full.model.predict))  -> df.mTOR
names(df.mTOR)[colnames(df.mTOR)=="Full.model.predict"] <- "predictResult" 
head(df.mTOR)

```

# summarize data 
```{r}

head(dt.Insulin)
head(dt.labeled)

# remove the identifier column since it is just a phosphorilation site  
dt.labeled$Identifier <- NULL
dt.labeled$Kinases <- as.factor(dt.labeled$Kinases) # only two classes so set as a factor
dt.labeled$Kinases <- as.numeric(dt.labeled$Kinases) # only two classes so set as a factor
dt.labeled$Seq.Window <- as.character(dt.labeled$Seq.Window) # convert from factor to character
head(dt.labeled)
str(dt.labeled)

```

# Analysize data 
# Attempt to do a pairwise plot of the features to understand if the labelled data is separable
```{r}

#install.packages("lattice")
#install.packages("ggplot2")

filteredFeatures <- c("Avg.Fold","AUC","Ins.1","Ins.2","LY","MK","Kinases")
temporalFeatures <- c("X15s","X30s","X1m","X2m","X5m","X10m","X20m","X60m","Kinases")

# try plotting the temporal data 
pairs(Kinases~., dt.labeled[,temporalFeatures], col=dt.labeled[,temporalFeatures]$Kinases)

# try plotting the other features 
# pair-wise scatterplots colored by class
pairs(Kinases~., dt.labeled[,filteredFeatures], col=dt.labeled[,filteredFeatures]$Kinases)

print("Plots appear to indicate that the labeled data can be separated. Thinking SVM should work.") 

```

# seq.window pattern transformation
```{r, warning=FALSE, echo=FALSE}

dt.labeled.mTOR <-  dt.Insulin[which(dt.Insulin$Kinases == 'mTOR'), ]
dt.labeled.Akt <- dt.Insulin[which(dt.Insulin$Kinases == 'Akt'), ]
dt.Insulin$Seq.Window <- as.character(dt.Insulin$Seq.Window)

# 7th Position is the phospho site
# total of 13 amino acids in the sequence window
phosphoSites.Akt.allSequences <- substr(dt.labeled.Akt$Seq.Window, 1,13)
phosphoSites.mTOR.allSequences <- substr(dt.labeled.mTOR$Seq.Window, 1,13)

# akt sequences
aktSequences <-  t(data.frame(strsplit(phosphoSites.Akt.allSequences, "")))
colnames(aktSequences) <- paste("", 1:13, sep = "")

# mtor sequences
mTORSequences <-  t(data.frame(strsplit(phosphoSites.mTOR.allSequences, "")))
colnames(mTORSequences) <- paste("", 1:13, sep = "")

# generate the consensus Matrix 

aktSequences.pfm <- consensusMatrix(phosphoSites.Akt.allSequences) #, as.prob = T)
colnames(aktSequences.pfm) <- colnames(aktSequences)

mTORSequences.pfm <- consensusMatrix(phosphoSites.mTOR.allSequences) # , as.prob = T) #as.prob = TRUE
colnames(mTORSequences.pfm) <- colnames(mTORSequences)

```

## visualise the patterns 

```{r, warning=FALSE}

# visualise the patterns
library(RColorBrewer)

par(mfrow=c(1, 1), mar=c(3, 3, 3, 3) + 0.1)
barplot(aktSequences.pfm, col=brewer.pal(nrow(aktSequences.pfm), "Paired"),
            legend.text = rownames(aktSequences.pfm),
            args.legend=list(x=ncol(aktSequences.pfm) + 5,y=max(colSums(aktSequences.pfm))+4,bty = "n"),
            main="Akt Sequence Window patterns")

par(mfrow=c(1, 1), mar=c(3, 3, 3, 3) + 0.1)
barplot(mTORSequences.pfm, col=brewer.pal(nrow(mTORSequences.pfm), "Paired"),
            legend.text = rownames(mTORSequences.pfm),
            args.legend=list(x=ncol(mTORSequences.pfm)+5,y=max(colSums(mTORSequences.pfm))+4,bty = "n"),
            main="mTOR Sequence Window patterns")

```

## pattern match score 

```{r}

# Pattern match score
# * Basic Formula  
#   + With the probabbility matrix (PSSM) it is possible to calculate a total score for a specific sequence
#   + The higher the score is, the higher is the probability that the sequence contains the searched motif.
#   + Convert the sequence window in the unlabelled dataset to a sum based on the Position-specific Scoring Matrix (PSSM)

library(seqLogo)

aktSequences.pfm <- consensusMatrix(phosphoSites.Akt.allSequences, as.prob = T)
colnames(aktSequences.pfm) <- colnames(aktSequences)

mTORSequences.pfm <- consensusMatrix(phosphoSites.mTOR.allSequences, as.prob = T) 
colnames(mTORSequences.pfm) <- colnames(mTORSequences)

# motif calculations function 
getMatchScore <- function(seq, PSSM) {
  x <- strsplit(x=seq,split='')
  #x
  #initialise vector to keep scores
  seq_score <- vector()
  #get the corresponding values from the PSSM
  for (i in 1:nchar(seq)){
    if (x[[1]][i] != "_") {
      seq_score[i] <- PSSM[x[[1]][i],i]
    }
    else seq_score[i] = 0.00
  }
  #seq_score
  sum(seq_score)
   
  #max score
  #sum(apply(mm,2,max))
  
}

getPercentageMatch <- function (score, max) {
  # normalise score to 0-1
  score / max
}

dt.Insulin$AktMotif <- as.numeric(lapply(dt.Insulin$Seq.Window, getMatchScore, PSSM=aktSequences.pfm))
dt.Insulin$mTORMotif <- as.numeric(lapply(dt.Insulin$Seq.Window, getMatchScore, PSSM=mTORSequences.pfm))

maxMotifAkt <- max(dt.Insulin$AktMotif)
minMotifAkt <- min(dt.Insulin$AktMotif)
maxMotifmTOR <- max(dt.Insulin$mTORMotif)
minMotifmTOR <- min(dt.Insulin$mTORMotif)

# normalised score
dt.Insulin$aktMotifMatch <- as.numeric(lapply(dt.Insulin$AktMotif, getPercentageMatch, max=maxMotifAkt))
dt.Insulin$mTORMotifMatch <- as.numeric(lapply(dt.Insulin$mTORMotif, getPercentageMatch, max=maxMotifmTOR))


```

# summary of the new features motif

```{r}
#View(dt.unlabeled)
head(dt.Insulin)
unique(dt.Insulin$Kinases)

max(dt.Insulin$AktMotif)
min(dt.Insulin$AktMotif)

max(dt.Insulin$mTORMotif)
min(dt.Insulin$mTORMotif)

dt.labeled.mTOR <-  dt.Insulin[which(dt.Insulin$Kinases == 'mTOR'), ]
dt.labeled.Akt <- dt.Insulin[which(dt.Insulin$Kinases == 'Akt'), ]
```

# feature extraction 

```{r, warning=FALSE}

# this part reference PengYi's code - KSP-PUEL GUI feature extraction ....
str(dt.Insulin)
dt.Insulin.feat <- dt.Insulin[, c("Identifier", "X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m")]
dt.Insulin.feat$X0s <- 0 
head(dt.Insulin.feat)

#### secondary feature 1
# magnitude calculation (mathematical mean)
average.score <- rowSums(dt.Insulin.feat[, 2:9]) / ncol(dt.Insulin.feat[, 2:9])
names(average.score) <- rownames(dt.Insulin.feat)
head(average.score)

#### secondary feature 2
# temporal profile fitting to check if the profile follows are good trend
fitting.score <- c()
for (i in 1:nrow(dt.Insulin)) {
   y <- as.numeric(dt.Insulin[i, 2:10]);
   x <- 2:10
   x2 = x^2
   lmfit <- lm(formula = y ~ x + x2 - 1)
   f.stat <- summary(lmfit)$fstatistic
   fitting.score <- c(fitting.score, f.stat[1])
}
fitted.score <- log2(fitting.score)
names(fitted.score) <- rownames(dt.Insulin.feat)

# combine extracted secondary features with primary features
dt.Insulin.feat <- cbind(fitted.score, dt.Insulin.feat) # removed avg score since same as avg fold
head(dt.Insulin.feat)

dt.Insulin <- merge(dt.Insulin, dt.Insulin.feat, by=c("Identifier", "X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m"))
dt.Insulin$X0s <- NULL
head(dt.Insulin)

# average fold and average score are the same. already provided

```

# model analysis and select the best fit model
## setup 
```{r}

# total 16 features approx
predictorsAkt <- c("X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m", "Avg.Fold", "AUC", "Ins.1", "LY", "Ins.2", "MK", "fitted.score","aktMotifMatch","Kinases")

predictorsmTOR <- c("X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m", "Avg.Fold", "AUC", "Ins.1", "LY", "Ins.2", "MK", "fitted.score","mTORMotifMatch","Kinases")

dt.labeled.mTOR <-  dt.Insulin[which(dt.Insulin$Kinases == 'mTOR'), ]
dt.labeled.Akt <- dt.Insulin[which(dt.Insulin$Kinases == 'Akt'), ]
dt.unlabelled <- dt.Insulin[which(is.na(dt.Insulin$Kinases)), ]

dt.feature.selection.akt <- dt.labeled.Akt[,predictorsAkt]
dt.feature.selection.akt$Kinases <- 1
dt.feature.selection.mTOR <- dt.labeled.mTOR[,predictorsmTOR]
dt.feature.selection.mTOR$Kinases <- 1

dim(dt.feature.selection.akt)
dim(dt.feature.selection.mTOR)

# pick 22 random for akt negatives
set.seed(55)
aktNeg <- sample(x=1:nrow(dt.unlabelled), size=nrow(dt.feature.selection.akt))
temp <- dt.unlabelled[aktNeg, predictorsAkt]
temp$Kinases <- -1
dt.feature.selection.akt <- rbind(dt.feature.selection.akt, temp)
temp <- NULL

# pick 26 random mtor negatives
set.seed(65)
mtorNeg <- sample(x=1:nrow(dt.unlabelled), size=nrow(dt.feature.selection.mTOR))
temp <- dt.unlabelled[mtorNeg, predictorsmTOR]
temp$Kinases <- -1
dt.feature.selection.mTOR <- rbind(dt.feature.selection.mTOR, temp)
temp <- NULL

# final sets for feature selection exercise
dt.feature.selection.akt$Kinases <- as.factor(dt.feature.selection.akt$Kinases)
dt.feature.selection.mTOR$Kinases <- as.factor(dt.feature.selection.mTOR$Kinases)
colnames(dt.feature.selection.akt)[17] <- "Class"
colnames(dt.feature.selection.mTOR)[17] <- "Class"

dim(dt.feature.selection.akt)
dim(dt.feature.selection.mTOR)

# function for model evaluation 

sampleData <- function (positives, featureSpace) {
      colnames(positives)[17] <- "Kinases" # change col name 
      featureSpace <- colnames(positives)
      #set.seed(33)
      randomsamples <- sample(nrow(dt.unlabelled), size=nrow(positives), replace = FALSE)
      #print(randomsamples)
      #print(randomsamples)
      negatives <- dt.unlabelled[randomsamples, featureSpace]
      negatives$Kinases <- -1
      # make a balanced set
      balanced <- rbind(positives, negatives)
      colnames(balanced)[17] <- "Class" # change col name
      return(balanced)
}

AnalyseModels <- function (method, data) {
  # prepare training scheme utilising LOOCV
  control <- trainControl(method="repeatedcv", number=10, repeats=3)
  controlLOOCV <- trainControl(method="LOOCV")
  
  model <- NULL
  
  #set.seed(5)
  switch(method, 
  kknn={
    model <- train(Class~., data=data, method=method, trControl=controlLOOCV)
  },
  svmRadial={
    tuneGrid <- expand.grid(sigma=seq(0.1,1,0.1),C=seq(0.1,5,0.5))
    model <- train(Class~., data=data, method=method, trControl=controlLOOCV,tuneGrid=tuneGrid)
  },
  svmLinear={
    tuneGrid <- expand.grid(C=seq(0.1,5,0.5))
    model <- train(Class~., data=data, method=method, trControl=controlLOOCV, tuneGrid=tuneGrid)
  },
  svmPoly={
    tuneGrid <- expand.grid(degree=(1:5),scale=seq(0.1,1,0.1),C=seq(0.1,5,0.5))
    model <- train(Class~., data=data, method=method, trControl=controlLOOCV, tuneGrid=tuneGrid)
  },
  {
    model <- train(Class~., data=data, method=method, trControl=controlLOOCV)
  }
  )
  #model <- train(Class~., data=data, method=method, trControl=controlLOOCV)
  print(model$bestTune)
  # resample data for testing and keep the same positives
  train <- sampleData(positives=data[which(data$Class == 1),])
  
  # predict on training set 
  pred <- predict(model$finalModel, train[,-17])
  if (method=="lda") {
    rocModel <- roc(train[, 17], as.numeric(pred$class))  
  }
  else {
    rocModel <- roc(train[, 17], as.numeric(pred))  
  }
  
  return(rocModel)
}

```

## run through some models to pick best one for AKT

```{r, warning=FALSE}

library(pROC)

#model <- train(Class~., data=dt.feature.selection.akt, method="svmRadial", trControl=controlLOOCV)

# analyze each model
rocKnn <- AnalyseModels("kknn",dt.feature.selection.akt) 
rocSvmR <- AnalyseModels("svmRadial",dt.feature.selection.akt)
rocSvmL <- AnalyseModels("svmLinear",dt.feature.selection.akt)
rocSvmP <- AnalyseModels("svmPoly",dt.feature.selection.akt)
rocLda <- AnalyseModels("lda",dt.feature.selection.akt)
#modelGbm <- AnalyseModels("gbm",dt.feature.selection.akt)

plot(rocKnn, legacy.axes = TRUE, col="red", lty=3, main="Akt model performances")
lines(rocSvmR, col="blue", lty=2)
lines(rocSvmL, col="green3", lty=1)
lines(rocSvmP, col="orange", lty=2)
lines(rocLda, col="purple", lty=3)

legend("bottomright", inset = .05, legend=c("KNN", "SVM Radial", "SVM Linear", "SVM Poly", "LDA"), col = c("red", "blue", "green3", "orange","purple"), lty=c(3, 2, 1, 2, 3))

```

## run through some models to pick best one for mTOR 

```{r, warning=FALSE}

# analyze each model
rocKnn <- AnalyseModels("kknn",dt.feature.selection.mTOR)
rocSvmR <- AnalyseModels("svmRadial",dt.feature.selection.mTOR)
rocSvmL <- AnalyseModels("svmLinear",dt.feature.selection.mTOR)
rocSvmP <- AnalyseModels("svmPoly",dt.feature.selection.mTOR)
rocLda <- AnalyseModels("lda",dt.feature.selection.mTOR)
#modelGbm <- AnalyseModels("gbm",dt.feature.selection.akt)


plot(rocKnn, legacy.axes = TRUE, col="red", lty=3, main="mTOR model performances")
lines(rocSvmR, col="blue", lty=2)
lines(rocSvmL, col="green3", lty=1)
lines(rocSvmP, col="orange", lty=2)
lines(rocLda, col="purple", lty=3)

legend("bottomright", inset = .05, legend=c("KNN", "SVM Radial", "SVM Linear", "SVM Poly", "LDA"), col = c("red", "blue", "green3", "orange","purple"), lty=c(3, 2, 1, 2, 3))

```

## Model analysis results
```{r}

print("SVM Radial yielded a the best overall result although the ROC curve indicates otherwise on the small subset")  

print("Best Model tuning parameters - AKT")
print("SVM Radial - Sigma: 1, C: 1.1")

print("Best Model tuning parameters - mTOR")
print("SVM Radial - Sigma: 1, C: 1.1")

```

# feature selection 
## setup
```{r, warning=FALSE}

library(mlbench)
library(caret)

# total 16 features approx
predictorsAkt <- c("X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m", "Avg.Fold", "AUC", "Ins.1", "LY", "Ins.2", "MK", "fitted.score","aktMotifMatch","Kinases")

predictorsmTOR <- c("X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m", "Avg.Fold", "AUC", "Ins.1", "LY", "Ins.2", "MK", "fitted.score","mTORMotifMatch","Kinases")

dt.labeled.mTOR <-  dt.Insulin[which(dt.Insulin$Kinases == 'mTOR'), ]
dt.labeled.Akt <- dt.Insulin[which(dt.Insulin$Kinases == 'Akt'), ]
dt.unlabelled <- dt.Insulin[which(is.na(dt.Insulin$Kinases)), ]

dt.feature.selection.akt <- dt.labeled.Akt[,predictorsAkt]
dt.feature.selection.akt$Kinases <- 1
dt.feature.selection.mTOR <- dt.labeled.mTOR[,predictorsmTOR]
dt.feature.selection.mTOR$Kinases <- 1

dim(dt.feature.selection.akt)
dim(dt.feature.selection.mTOR)

# pick 22 random for akt negatives
aktNeg <- sample(x=1:nrow(dt.unlabelled), size=nrow(dt.feature.selection.akt))
temp <- dt.unlabelled[aktNeg, predictorsAkt]
temp$Kinases <- -1
dt.feature.selection.akt <- rbind(dt.feature.selection.akt, temp)
temp <- NULL

# pick 26 random mtor negatives
mtorNeg <- sample(x=1:nrow(dt.unlabelled), size=nrow(dt.feature.selection.mTOR))
temp <- dt.unlabelled[mtorNeg, predictorsmTOR]
temp$Kinases <- -1
dt.feature.selection.mTOR <- rbind(dt.feature.selection.mTOR, temp)
temp <- NULL

# final sets for feature selection exercise
dim(dt.feature.selection.akt)
dim(dt.feature.selection.mTOR)
```

## functions for feature selection 
```{r}

tstatSort <- function (train) {
  train.byClass <- split(train[,-17], train$Kinases)

  # perform a t-test
  feature.pvalues <- c()
  for(i in 1:(ncol(train)-1)) {
    feature.pvalues <- c(feature.pvalues, t.test(train.byClass[[1]][,i], train.byClass[[2]][,i])$p.value)
  }
  names(feature.pvalues) <- colnames(train[,-17])
  
  # filter the top most discriminative feature based on p-values
  filtered.features <- names(sort(feature.pvalues)) #[1:10])
  filtered.features
}

backwardStepwise <- function(train, cls.train, sortedFeatures) {

  test.acc <- c() 
  
  # carry out a LOOCV
  set.seed(99)
  fold <- createFolds(cls.train, k=nrow(train))
  
  # remove the least useful predictor one at a time
  for (i in length(sortedFeatures):1) {
	  current.f <- colnames(train)[i]
	  features <- sortedFeatures[1:i]
	  #print(features)
	  
    test.accuracies <- c()
	  
	  for(i in 1:length(fold)) {
        truth <- cls.train[fold[[i]]]
        trainingData <- train[-fold[[i]], features]
        testingData <- train[fold[[i]],features]
        #print(truth)
        svm.model <- svm(x=trainingData, y=cls.train[-fold[[i]]], kernel="radial", type="C-classification")
        pred <- predict(svm.model, testingData)
        #print(pred)
        accuracy <- sum(pred == truth) / length(truth)
        test.accuracies <- c(test.accuracies, accuracy)
	  }
	  
	  # take the avg test accuracy as benchmark 
    test.acc <- c(test.acc, mean(test.accuracies))
	  
  }
  
  return(test.acc)
}

# run the random sampling XX times to see if the result is different
randomAccuracies <- function(positives, featureSpace, sortedFeatures, iterations) {
  accuracies.all <- data.frame()
  #names(accuracies.all) <- seq(length(sortedFeatures), 1)
  for (i in 1:iterations) {
    # pick 22 random for negatives
    set.seed(i)
    randomsamples <- sample(nrow(dt.unlabelled), size=nrow(positives), replace = FALSE)
    #print(randomsamples)
    negatives <- dt.unlabelled[randomsamples, featureSpace]
    negatives$Kinases <- -1
    # make a balanced set
    balanced <- rbind(positives, negatives)
    # run the wrapper selection process
    accuracies <- backwardStepwise(train = balanced[,-17],
                                        cls.train = balanced[,17],
                                        sortedFeatures = sortedFeatures)
    #accuracies <- data.frame(accuracies)
    #names(accuracies) <- seq(length(sortedFeatures), 1)
    accuracies.all <- rbind(accuracies.all, accuracies)
  }
  names(accuracies.all) <- seq(length(sortedFeatures), 1)
  return(accuracies.all)
}


```

## run the wrapper feature selection (SVM)

```{r}

# carry out t-test to sort features
featuresAkt <- tstatSort(dt.feature.selection.akt)
# sorted features 
featuresAkt
# best feature as per t-test
featuresAkt[1]

# carry out t-test to sort features
featuresmTOR <- tstatSort(dt.feature.selection.mTOR)
# sorted features 
featuresmTOR
# best feature as per t-test
featuresmTOR[1]

```

## backward stepwise 
### akt feature selection with random sampling, LOOCV 

```{r}

# reset the positives
dt.feature.selection.akt <- dt.labeled.Akt[,predictorsAkt]
dt.feature.selection.akt$Kinases <- 1
dt.feature.selection.mTOR <- dt.labeled.mTOR[,predictorsmTOR]
dt.feature.selection.mTOR$Kinases <- 1

test.accuracies.akt <- randomAccuracies(positives = dt.feature.selection.akt, 
                                        featureSpace = predictorsAkt,
                                        sortedFeatures = featuresAkt,
                                        iterations = 100)

```

### mTOR feature selection with random sampling

```{r}

test.accuracies.mtor <- randomAccuracies(positives = dt.feature.selection.mTOR, 
                                        featureSpace = predictorsmTOR,
                                        sortedFeatures = featuresmTOR,
                                        iterations = 100)
```

## plot the results for backwards selection 

```{r}

# avg the results 
test.accuracies.akt.avg <- colMeans(test.accuracies.akt)
test.accuracies.mtor.avg <- colMeans(test.accuracies.mtor)

```

### akt plots

```{r}

plot(rev(test.accuracies.akt.avg),type="l",ylim=c(0.8, 1), xaxt="n", ylab="Test accuracies", xlab="Number of features sorted",main="Feature selection vs model accuracy akt")
axis(1, at=seq(1, 16, by = 1), las=2)

```

### mtor plots

```{r}

plot(rev(test.accuracies.mtor.avg),type="l",ylim=c(0.5, 1), xaxt="n", ylab="Test accuracies", xlab="Number of features sorted",main="Feature selection vs model accuracy mtor")
axis(1, at=seq(1, 16, by = 1), las=2)

```
## feature selection results 

All the features in both Akt & Mtor classifications yields impressive results. 
As such all features are important and cannot be dropped.

# Let there be light - Learn more about the unlabelled dataset

The purpose here is to identify highly probable negatives for mTORs and Akts respectively so that an ensemble can be built more effectively. 

Successful identificaiton of the negative samples will assist in removing bias and also solve the class imbalance problem. 

## helper functions/methods

```{r, warning=FALSE}

library(MASS)

# helpers for ensemble/bagging

# function to build model 
model.func <- function (dt.model.full, dt.labeled, cost.n, degree.n, iterations, dt.pred) {

  # instantiate the last probability values 
  lastProbabilities <- rep(0, nrow(dt.model.full))
  iterationCors <- c()
  
  dt.model.full %>%
    mutate(Class = -1) %>%
    mutate(Class = replace(Class, Identifier %in% dt.labeled$V1, 1)) -> dt.model.full
  
  # data frame to store pred result
  dt.result <- dt.model.full[, c("Identifier", "Class")]
  
  for (i in 1:iterations) {
    
    dt.nolabel.sample <- dt.model.full[dt.model.full$Class==-1,][sample(nrow(dt.model.full[dt.model.full$Class==-1, ]), nrow(dt.labeled)),]
    
    dt.model <- rbind(dt.model.full[dt.model.full$Class==1,], dt.nolabel.sample)
    
    # build SVM classification model incl probability
    fit.model <- svm(Class ~ ., data=dt.model[, 2:ncol(dt.model)], kernel="radial", type="C-classification", cost=cost.n, degree=degree.n, decision.values = TRUE, probability=TRUE)
    
    pred <- predict(fit.model, dt.pred, decision.values = TRUE, probability = TRUE)
    
    currentProbabilities <- attr(pred, "probabilities")[,1] # attr(pred, "probabilities")[,1]
    dt.result[, ncol(dt.result) + 1] <- currentProbabilities
    names(dt.result)[ncol(dt.result)] <- paste0("model_", i)
    
    # calculate the correlations and store 
    currentCor <-  cor(currentProbabilities, lastProbabilities)
    iterationCors <- c(iterationCors, currentCor)  
    lastProbabilities <- currentProbabilities
  
  }
    
  dt.final <- data.frame(dt.result[,1:2],pred.Means=rowMeans(dt.result[,3:ncol(dt.result)]))
  names(dt.final)[colnames(dt.final)=="pred.Means"] <- "predictResult"
  
  # plot the result of the iterations 
  plot(iterationCors,type="l",#ylim=c(0, 2),
       ylab="correlation", 
       xlab="Number of iterations",
       main="Optimisation point for random sampling")
  
  return(dt.final)
  
}

# ensemble function (make final predictions)
model.adp2.func <- function (dt.model.full, dt.labeled, dt.neg, cost.n, degree.n, dt.pred) {

  dt.model.full %>%
    mutate(Class = -1) %>%
    mutate(Class = replace(Class, Identifier %in% dt.labeled$V1, 1)) -> dt.model.full

  dt.neg$Class = -1
  
  # data frame to store pred result
  dt.result <- dt.model.full[, c("Identifier", "Class")]

  for (i in 1:1000) {

    # generate data for model, combining positive labeled, and the same size selected from most likely negative from adaptive sampling result
    dt.model <- rbind(dt.model.full[dt.model.full$Class==1,], dt.neg[sample(nrow(dt.neg), nrow(dt.labeled)),])

    # build SVM classification model incl probability
    fit.model <- svm(Class ~ ., data=dt.model[, 2:ncol(dt.model)], kernel ="radial", type="C-classification", decision.values = TRUE, probability=TRUE, cost = cost.n, degree = degree.n)

    pred <- predict(fit.model, dt.pred, decision.values = TRUE, probability = TRUE)

    dt.result[, ncol(dt.result) + 1] <- attr(pred, "probabilities")[,1]
    names(dt.result)[ncol(dt.result)] <- paste0("model_", i)

  }
  dt.final <- data.frame(dt.result[,1:2],pred.Means=rowMeans(dt.result[,3:ncol(dt.result)]))
  names(dt.final)[colnames(dt.final)=="pred.Means"] <- "predictResult"

  return(dt.final)

}
```

## function to analyze difference between 2016 predict result and our prediction result 
```{r}
diff.func <- function (type, dt.original, dt.prediction, title) {

# bootstrap sample statistic (difference in prediction probability)
B=1000  
dt.original.B <- rep(0,B)
dt.prediction.B <- rep(0,B)

for ( i in 1:B ) {
  dt.original.B[i] = mean(sample(dt.original[, grep(type, colnames(dt.original))], size = nrow(dt.original), replace = TRUE), na.rm = TRUE)
  dt.prediction.B[i] = mean(sample(dt.prediction[, grep(type, colnames(dt.prediction))], size = nrow(dt.prediction), replace = TRUE), na.rm = TRUE)
}

b.diff<-(dt.original.B-dt.prediction.B)*100

print(paste("95% Confidence Interval of Average(Bootstrap) difference in ", type, round(quantile(b.diff,0.05),2), round(quantile(b.diff,0.95),2)))

dt.delta <- merge(dt.original[, c(which(colnames(dt.original)=="Identifier"), grep(type, colnames(dt.original)))], dt.prediction[, c(which(colnames(dt.prediction)=="Identifier"), grep(type, colnames(dt.prediction)))], by=("Identifier"))

dt.delta$delta<-(dt.delta[,2]-dt.delta[,3])*100
#dt.delta<-dt.delta[complete.cases(dt.delta$delta),] 

ggplot(dat=dt.delta) + geom_histogram(aes(x=delta),bins=50) +ggtitle(paste0(title, " Distribution of prediction difference 2016 minus 2017"))

}

```

## Create Highly probabble Negative subsets

### akt

```{r}

dt.Akt.fulmodel <- dt.Insulin[, c("Identifier", "X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m", "Avg.Fold", "AUC", "Ins.1", "LY", "Ins.2", "MK", "AktMotif", "fitted.score")]
aktNancies <- model.func(dt.Akt.fulmodel, dt.Akt, 1.1, 1, 1000, dt.Akt.fulmodel[, 2:17]) # as per best tuning param val 

```

### mtor 

```{r}

dt.mTOR.fulmodel <- dt.Insulin[, c("Identifier", "X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m", "Avg.Fold", "AUC", "Ins.1", "LY", "Ins.2", "MK", "mTORMotif", "fitted.score")]
mTORNancies <- model.func(dt.mTOR.fulmodel, dt.mTOR, 1.1, 1, 1000, dt.mTOR.fulmodel[, 2:17])

```

### dataset for ensemble

```{r}

# akts 
aktNancies[aktNancies$Class==1, ]
aktNancies[aktNancies$Identifier %in% dt.mTOR$V1, ]

# negatives subset for ensemble step 
Akt.neg <- dt.Akt.fulmodel[which(dt.Akt.fulmodel$Identifier %in% 
                                   aktNancies[aktNancies$predictResult < 0.5, ]$Identifier 
                                 & !dt.Akt.fulmodel$Identifier %in% dt.Akt$V1), ]

# mtors 
mTORNancies[mTORNancies$Class==1, ]
mTORNancies[mTORNancies$Identifier %in% dt.Akt$V1, ]

# negative subset for ensemble step, due to high probability value of labeled Akt in mTOR model, we decided to set up threshold at 0.8 for negative class in mTOR model
mTOR.neg <- dt.mTOR.fulmodel[which(dt.mTOR.fulmodel$Identifier %in%
                                     mTORNancies[mTORNancies$predictResult 
                                                          < 0.8, ]$Identifier & 
                                     !dt.mTOR.fulmodel$Identifier %in% dt.mTOR$V1), ]

```


# Final Predictions
```{r, warning=FALSE}

Akt.adp2 <- model.adp2.func(dt.Akt.fulmodel, dt.Akt, Akt.neg, 1.1, 1, dt.Akt.fulmodel[, 2:17])
Akt.adp2[Akt.adp2$Class==1, ]
Akt.adp2[Akt.adp2$Identifier %in% dt.mTOR$V1, ]


mTOR.adp2 <- model.adp2.func(dt.mTOR.fulmodel, dt.mTOR, mTOR.neg, 1.1, 1, dt.mTOR.fulmodel[, 2:17])
mTOR.adp2[mTOR.adp2$Class==1, ]
mTOR.adp2[mTOR.adp2$Identifier %in% dt.Akt$V1, ]

print("Our final Akt prediction result with adaptive sampling technic is saved in data frame Akt.adp2")
head(Akt.adp2)
print("Our final mTOR prediction result with adaptive sampling technic is saved in data frame mTOR.adp2")
head(mTOR.adp2)

```

# compares prediction diff
## 1. use bootstraping to calculate confidence interval of average difference 
## 2. generates a plot to show distribution of difference 

## comparison with 2016 
```{r}

diff.func ("predictResult", df.Akt, Akt.adp2, "Akt Adaptive Sampling Model - ")
diff.func ("predictResult", df.mTOR, mTOR.adp2, "mTOR Adaptive Sampling Model - ")

```

```{r}

# result without adaptive sampling 
diff.func ("predictResult", df.Akt, aktNancies, "Akt predictions w/o adaptive sampling - ")
diff.func ("predictResult", df.mTOR, mTORNancies, "mTOR predictions w/o adaptive sampling - ")

```


# using simulation and/or create a conservative lower and/or upper bound for performance
```{r}
# summarize each feature
feat.list <- dt.Insulin[, c("X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m", "Avg.Fold", "AUC", "Ins.1", "LY", "Ins.2", "MK", "AktMotif", "mTORMotif", "fitted.score")]
feat.summary <- sapply(feat.list, summary)
feat.sd <- summarise_all(feat.list, funs(sd))
row.names(feat.sd) <- "sd"

feat.summary <- rbind(feat.summary, feat.sd)
#View(feat.summary)

# plot each feature
par(mfrow=c(2, 2))

for (i in 1:ncol(feat.list)) {
  
  x <- feat.list[, i]
  h<-hist(x, breaks=30, col="red", xlab=colnames(feat.summary)[i], main=paste0("Histogram with Normal Curve - ", colnames(feat.summary)[i]))
  xfit<-seq(min(x),max(x),length=40)
  yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
  yfit <- yfit*diff(h$mids[1:2])*length(x)
  lines(xfit, yfit, col="blue", lwd=2)
}

print("We can see all features are normally distributed, we can generate simulation data set based on distribution and summary of each feature")

# generate simulation data 
dt.simulation.func <- function(dt){
  
  fullset <- NULL
  Identifier <- seq.int(1, 100000, 1)
  fullset <- cbind(fullset, Identifier)
  
  for (i in 1:ncol(dt)) {
    set.seed(123)
    x <- rnorm(n=100000, m=dt[c("Mean"), i], sd=dt[c("sd"), i])
    col <- x[x>=dt[c("Min."), i] & x<=dt[c("Max."), i]] 
    fullset <- cbind(fullset, col)
  }
  return(fullset)
}

dt.sim <- dt.simulation.func(feat.summary)
dt.sim <- data.frame(dt.sim)
colnames(dt.sim) <- c("Identifier", "X15s", "X30s", "X1m", "X2m", "X5m", "X10m", "X20m", "X60m", "Avg.Fold", "AUC", "Ins.1", "LY", "Ins.2", "MK", "AktMotif", "mTORMotif", "fitted.score")
#class(dt.sim)
#sapply(dt.sim, class)
#View(dt.sim)
#ncol(dt.sim)
#ncol(feat.summary)

# check if any missing value in simulation data set 
sum(!complete.cases(dt.sim))

dt.simulation.Akt <- dt.sim[sample(1:nrow(dt.sim), size = nrow(dt.Insulin)), -which(names(dt.sim) == "mTORMotif")]
dt.simulation.mTOR <- dt.sim[sample(1:nrow(dt.sim), size = nrow(dt.Insulin)), -which(names(dt.sim) == "AktMotif")]

# final prediction for simulated data 
Akt.sim.adp2 <- model.adp2.func(dt.Akt.fulmodel, dt.Akt, Akt.neg, 1.1, 1, dt.simulation.Akt )

mTOR.sim.adp2 <- model.adp2.func(dt.mTOR.fulmodel, dt.mTOR, mTOR.neg, 1.1, 1, dt.simulation.mTOR)

# plot given insulin data prob and simulation data prob for Akt 
par(mfrow=c(1, 2))
x <- Akt.sim.adp2[, c("predictResult")]
h<-hist(x, breaks=30, col="red", xlab="Predict Result", main="Akt Simulation Data")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)

x <- Akt.adp2[, c("predictResult")]
h<-hist(x, breaks=30, col="red", xlab="Predict Result", main="Akt Given Data")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)

print("From the plot, we can see Akt prediction results for both simulation data and actual data are following the similar distribution curve ")

# plot given insulin data prob and simulation data prob for mTOR 
par(mfrow=c(1, 2))
x <- mTOR.sim.adp2[, c("predictResult")]
h<-hist(x, breaks=30, col="red", xlab="Predict Result", main="mTOR Simulation Data")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)

x <- mTOR.adp2[, c("predictResult")]
h<-hist(x, breaks=30, col="red", xlab="Predict Result", main="mTOR Given Data")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)

print("From the plot, we can see mTOR prediction results for both simulation data and actual data are following the similar distribution curve ")

```

# Output session information
```{r}
sessionInfo()
```

