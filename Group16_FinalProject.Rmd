---
title: "Group16_Project2"
group: 16
member: Nagib Shah / Ling Qi / Xinan Ma / Young Choi
author: "Ling Qi"
date: "19 September 2017"
output: html_document
---

# All group members, please install packages here
```{r}
install.packages("tibble") # this is for RSSL
install.packages("RSSL")
install.packages("upclass")
install.packages("mclust")
install.packages("caret")
install.packages("lme4")
install.packages("randomForest")

```

# All group members, please set up your root working directory here
```{r}
require(knitr)
opts_knit$set(root.dir = "C:/Users/Ling/Documents/My Study/STAT5003/Project2/Final project/Datasets")

```

# All group members, please call libraries here
```{r}
library(mclust)
library(upclass)
library(RSSL)
library(e1071)
library(caret)
library(randomForest)
library(dplyr)
```

# All group members, please declare constant variables here
```{r}

```

# Read data
```{r}
dt.Insulin <- read.delim("InsulinPhospho.txt")
dt.Akt <- read.delim("Akt_substrates.txt", header = F)
dt.mTOR <- read.delim("mTOR_substrates.txt", header = F)   
dim(dt.Akt)
dim(dt.mTOR)

dt.Insulin.New <- dt.Insulin
#dt.Insulin.New$Class <- NA 

dt.Insulin.New$Class[dt.Insulin.New$Identifier %in% dt.Akt$V1] <- 0
dt.Insulin.New$Class[dt.Insulin.New$Identifier %in% dt.mTOR$V1] <- 1

dt.Akt.labeled <- dt.Insulin.New[which(dt.Insulin.New$Identifier %in% dt.Akt$V1),]
dt.mTOR.labeled <- dt.Insulin.New[which(dt.Insulin.New$Identifier %in% dt.mTOR$V1),]

dt.All.unlabeled <- subset(dt.Insulin.New, is.na(dt.Insulin.New$Class))
dt.All.labeled <- subset(dt.Insulin.New, !is.na(dt.Insulin.New$Class))


#View(dt.All.labeled.noID)
#dim(dt.All.unlabeled)
#View(dt.All.unlabeled.noID)
#dim(dt.All.labeled)
#View(dt.All.labeled)

#View(dt.Insulin.New)
#unique(dt.Insulin.New$Class)
#View(dt.Insulin)
#View(dt.Akt)
#View(dt.mTOR)
#View(dt.Akt.labelled)
#View(dt.mTOR.labelled)
```


# self-training approach - iterative approach
# approach 1 - build classification SVM model, 
# result no good including sequence window -no Akt , all mTOR
# result good without sequence window - so need to think about to transform sequence window to sort of probability of similarity
```{r}
# SVM for all labeled data, identifier should not be feature, ignore it when build model
model <- svm(Class ~ ., data=dt.All.labeled[, 2:17], type="C-classification")
summary(model)
pred <- predict(model,dt.All.labeled[, 2:16])
table(pred,dt.All.labeled[, 17])

# take out of sequence window, do the classification SVM model again 
model.noSW <- svm(Class ~ ., data=dt.All.labeled[, 3:17], type="C-classification", probability=TRUE)
summary(model.noSW)
pred.noSW <- predict(model.noSW,dt.All.labeled[, 3:16])
table(pred.noSW, dt.All.labeled[, 17])

#head(dt.All.labeled[, 3:17])

# predict unlabeled data 
pred.unlabeled.noSW <- predict(model.noSW,dt.All.unlabeled[, 3:16], decision.values = TRUE, probability=TRUE)
pred.unlabeled.noSW
head(attr(pred.unlabeled.noSW, "probabilities"))

pred <- predict(model, x, decision.values = TRUE, probability = TRUE)
attr(pred.unlabeled.noSW, "decision.values")
attr(pred.unlabeled.noSW, "probabilities")[1:4,]
attr(pred.unlabeled.noSW, "probabilities")[,1]


```

```{r}

pred.conv <- function(threshold, a, b) {
  pred.table <- cbind(a, b)
  pred.table <- as.data.frame.matrix(pred.table)

  pred.table$predictClass <- 0
  pred.table[pred.table$prediction > threshold, "predictClass"] <- 1

  colnames(pred.table) <- c("OriginalType", "prediction", "predictClass")
}

```

# approach 2
# step 1 - build regression SVM model based on labled data 
```{r}

# classification model
ml <- svm(as.factor(Class)~., data = dt.All.labeled[, 2:17])
summary(ml)
table(predict(ml, dt.All.labeled[, 2:16]), dt.All.labeled$Class)


# regression model
model.SVM <- svm(Class~., data = dt.All.labeled[, 2:17])
#model.SVM <- svm(as.factor(Class)~., data = dt.All.labeled.noID)
summary(model.SVM)

# test accuracy for labled data
prediction <- predict(model.SVM, dt.All.labeled[, 2:16])
#View(dt.All.labeled[, 2:16])
#View(prediction)
#View(cbind(dt.All.labeled[,17], prediction))

pred.table <- cbind(dt.All.labeled[,17], prediction)
#pred.table <- table(prediction, dt.All.labeled$Class)
class(pred.table)
dim(pred.table)
pred.table <- as.data.frame.matrix(pred.table)

#sort pred table by prediction 
pred.table <- pred.table[order(pred.table$prediction), ]


# we can see when prob > 0.92, the classification is more accurate
#View(pred.table[pred.table$prob > 0.92,][, 1:2])
#View(data.frame(table(pred.table[pred.table$prob > 0.92,][, 1:2])))
#pred.table %>% mutate(predictClass = ifelse(prediction > 0.92, 1, 0))

pred.table$predictClass <- 0
pred.table[pred.table$prediction > 0.92, "predictClass"] <- 1

colnames(pred.table) <- c("OriginalType", "prediction", "predictClass")

View(pred.table)
data.frame(table(pred.table[, c("OriginalType", "predictClass")]))

# we can see when we set up threshold to 0.92, there are only 4 Akt are classified as mTOR, all mTOR are classified correctly

# below is for test only
# classification model split data as traiin and test
sample <- sample.int(n=nrow(dt.All.labeled), size=floor(0.8*nrow(dt.All.labeled)), replace=F)
train <- dt.All.labeled[sample, ]
test <- dt.All.labeled[-sample, ]

model <- svm(Class~., data = train[, 2:17])
summary(model)

# test accuracy for test data
pred <- predict(model, test[, 2:16])
table(pred, test$Class)

```


# approach 2
# step 2 - use the same model to predict unlabled data, and set up threshold to 0.92
```{r}
prediction.unlabeled <- predict(model.SVM, dt.All.unlabeled[, 2:16])

pred.table.unlabled <- cbind(dt.All.unlabeled[,17], prediction.unlabeled)
pred.table.unlabled <- as.data.frame.matrix(pred.table.unlabled)

#sort pred table by prediction 
pred.table.unlabled <- pred.table.unlabled[order(pred.table.unlabled$prediction), ]

#set up threshold 0.92
pred.table.unlabled$predictClass <- 0
pred.table.unlabled[pred.table$prediction > 0.92, "predictClass"] <- 1

colnames(pred.table.unlabled) <- c("OriginalType", "prediction", "predictClass")

View(pred.table.unlabled)
data.frame(table(pred.table.unlabled[, c("OriginalType", "predictClass")]))

```











# approach 3 - use naive bayes classification build model based on 60% labeled data, then to predict 40% labeled data, then build self training model
# this approach failed
```{r}

func <- function(m,d) {
   p <- predict(m,d,type='raw')
   data.frame(cl=colnames(p)[apply(p,1,which.max)],p=apply(p,1,max))
}

# Set Seed so that same sample can be reproduced in future also
set.seed(101) 

# Now Selecting 60% of labeled data and combine with unlabeled data   
sample <- sample.int(n = nrow(dt.Akt.labeled), size=floor(.6*nrow(dt.Akt.labeled)), replace = F)
dt.Akt.labeled.train <- dt.Akt.labeled[sample, ]
dt.Akt.labeled.test <- dt.Akt.labeled[-sample, ]
#View(dt.Akt.labeled.train)
#dim(dt.Insulin)
#dim(dt.Akt.labeled)

dt.Akt.train <- rbind(dt.Akt.labeled.train, dt.Insulin.New[!(dt.Insulin.New$Identifier %in% dt.Akt$V1),])


nbAkt <- naiveBayes(Class ~ ., dt.Akt.labeled.train)
summary(nbAkt)

table(predict(nbAkt,dt.Akt.labeled.test),dt.Akt.labeled.test$Class)
nbAktST <- SelfTrain(Class ~ .,dt.Akt.train,learner('naiveBayes',list()),'func')
table(predict(nbAktST,dt.Akt.labeled.test),dt.Akt.labeled.test$Class)

```
# approach 4 - linear separation  
```{r}

svm.linear <- svm(Class ~ ., data=dt.Insulin.New, kernel = 'linear', type ='C-classification')
summary(svm.linear)

```

# approach 4a - 10 folder cross validation - got error
```{r}


# define training control - uses 10-fold cross validation with 3 repeats
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7

metric <- "Accuracy"

#unique(dt.Insulin.New$Class)

# SVM Linear
set.seed(seed)
fit.svmLinear <- train(Class~., data=dt.Insulin.New, method="svmLinear", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)

# SVM Polynomial
set.seed(seed)
fit.svmPolynomial <- train(Class~., data=dt.Insulin.New, method="svmPoly", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)

# SVM Radial
set.seed(seed)
fit.svmRadial <- train(Class~., data=dt.Insulin.New, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)

# Linear Discriminant Analysis
set.seed(seed)
fit.lda <- train(Class~., data=dt.Insulin.New, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)

results <- resamples(list(svmL=fit.svmLinear, svmP=fit.svmPolynomial, svmR=fit.svmRadial, svmLda=fit.lda))
# Table comparison
summary(results)

# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)
```

# 
```{r}
library(RSSL)
library(dplyr,warn.conflicts = FALSE)
library(ggplot2,warn.conflicts = FALSE)

# Train classifier
g_nm <- NearestMeanClassifier(Class~.,dt.Insulin.New)
g_self <- SelfLearning(Class~.,dt.Insulin.New,
                       method=NearestMeanClassifier,
                       prior=matrix(0.5,2))

# Plot dataset
df %>% 
  ggplot(aes(x=X1,y=X2,color=Class,size=Class)) +
  geom_point() +
  coord_equal() +
  scale_size_manual(values=c("-1"=3,"1"=3), na.value=1) +
  geom_linearclassifier("Supervised"=g_nm,
                  "Semi-supervised"=g_self)
```

# separate labelled data as 60/40, combine 60% labeled with unlabeled data to train model, and use the 40% labeled for testing
```{r}
# Set Seed so that same sample can be reproduced in future also
set.seed(101) 

# Now Selecting 60% of labeled data and combine with unlabeled data   
sample <- sample.int(n = nrow(dt.Akt.labeled), size=floor(.6*nrow(dt.Akt.labeled)), replace = F)
dt.Akt.labeled.train <- dt.Akt.labeled[sample, ]
dt.Akt.labeled.test <- dt.Akt.labeled[-sample, ]
View(dt.Akt.labeled.train)
#dim(dt.Insulin)
#dim(dt.Akt.labeled)

dt.Akt.train <- rbind(dt.Akt.labeled.train, dt.Insulin[!(dt.Insulin$Identifier %in% dt.Akt$V1),])

dim(dt.Akt.train)
dim(dt.Akt.labeled.test)


```

